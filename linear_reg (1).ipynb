{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "29a1fae5-3d99-4826-ad3f-f10d72aa2e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3bb129d-9871-45e7-9ff4-ed9bb59135bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear Regression\n",
    "# Now we are NOT using sklearn, we are building Linear Regression from scratch.\n",
    "\"\"\"\n",
    "You are creating your own Linear Regression model,\n",
    "training it using Gradient Descent,\n",
    "and then predicting values.\n",
    "\n",
    "Just like sklearnâ€™s LinearRegression, but you wrote it yourself.\n",
    "\"\"\"\n",
    "\n",
    "# Create your own LinearRegression class\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01,n_iter = 1000):  # Runs automatically when model is created.\n",
    "        self.bias = None\n",
    "        self.weights = None\n",
    "        self.lr = learning_rate\n",
    "        self.n_iter = n_iter # how  many time trainig runs\n",
    "\n",
    "    def fit(self,X,y): # here we are trainig the funx\n",
    "        m,n = X.shape # ( number of samples , number of features) , m is no of rows, n is no of column\n",
    "        \n",
    "        # step 1 - initialize params\n",
    "        self.bias = 0\n",
    "        self.weights = np.zeros(n)\n",
    "\n",
    "        # Gradient Descent\n",
    "        for i in range(self.n_iter): # runs 1000 time for better performance\n",
    "            # step 2 - calc y_pred\n",
    "            y_pred = self.bias + np.dot(X,self.weights)\n",
    "    \n",
    "            # step 3 - calc gradiant\n",
    "            db = (1/m) * np.sum(y_pred - y)\n",
    "            dw = (1/m) * np.dot(X.T, (y_pred - y))\n",
    "    \n",
    "            # step 4 - convergence, theorem - params update\n",
    "            self.bias -= self.lr * db\n",
    "            self.weights -= self.lr * dw \n",
    "            \n",
    "    def predict(self, X):\n",
    "        y_pred = self.bias + np.dot(X,self.weights)\n",
    "        return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e959b9b-97c1-4b7f-9942-ac3aaa7c2b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.06850809 4.04226297 6.01601785 7.98977273 9.96352761]\n",
      "0.09475321533750963\n",
      "[1.97375488]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1] ,[2] ,[3],[4],[5]])\n",
    "y = np.array([2,4,6,8,10])\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X,y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print(y_pred) \n",
    "\n",
    "print(model.bias)\n",
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b492be2e-4083-462c-854b-96067dcece7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linear regression with OLS\n",
    "\"\"\"\n",
    "| Method                    | How it learns              |\n",
    "| ------------------------- | -------------------------- |\n",
    "| **Gradient Descent**      | Learns slowly step-by-step |\n",
    "| **OLS (Normal Equation)** | Solves directly using math |\n",
    "\n",
    "OLS is use when there is small data and less feature whereas Gradient Descent is use where large data \n",
    "\"\"\"\n",
    "class LinearRegressionOLS:\n",
    "    def __init__(self):\n",
    "        self.bias = None\n",
    "        self.weights = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        m, n = X.shape\n",
    "\n",
    "        X_b = np.c_[np.ones((m, 1)), X]\n",
    "\n",
    "        # normal equation\n",
    "        # theta = np.dot( np.linealg.inv(np.dot(X_b.T, X_b)), np.dot(X_b.T, y)) \n",
    "        theta = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "\n",
    "        self.bias = theta[0]\n",
    "        self.weights = theta[1:]\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_pred = self.bias + np.dot(X, self.weights)\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1eb85d0e-5e83-4fcd-a0fd-da4fbf0e0ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 2.  4.  6.  8. 10.]\n",
      "5.329070518200751e-15\n",
      "[2.]\n"
     ]
    }
   ],
   "source": [
    "# same copy of above code here \n",
    "\n",
    "X = np.array([[1] ,[2] ,[3],[4],[5]])\n",
    "y = np.array([2,4,6,8,10])\n",
    "\n",
    "model = LinearRegressionOLS()\n",
    "model.fit(X,y)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "print(y_pred) \n",
    "\n",
    "print(model.bias)\n",
    "print(model.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fb0f06-2543-4c0d-9796-999f0b1670d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
